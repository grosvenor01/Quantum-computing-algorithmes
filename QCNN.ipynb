{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# buidling quantum CNN circuit\n",
        "1. **transforme data to quantum states**\n",
        "\n",
        "2. **transfrome Classical CNN model to Quantum CNN**\n",
        "\n",
        "3. **add multi scale renormalization anstaz** : lhaja lowla hna c ndirou short range disentangler b CNOT nahiw biha el uneccecary entanglement bin consucutive qubits , apres napplyaiw RY gate bach nfusioniw two qubits l one larger information (qubits ytkhyrou 3la hssab architecture ta3 mera) , ouraha CNOT whdokhr ynahi el entanglement bin larger infos hadouma ,\n",
        "\n",
        "4. **add quantum error correction** :\n",
        "\n",
        "5. **test** :\n"
      ],
      "metadata": {
        "id": "qGufms9xSiTp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pennylane"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v7Q_13x2cKRu",
        "outputId": "1e8f659a-e535-4126-8872-3c5f4370775e"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pennylane in /usr/local/lib/python3.11/dist-packages (0.40.0)\n",
            "Requirement already satisfied: numpy<2.1 in /usr/local/lib/python3.11/dist-packages (from pennylane) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from pennylane) (1.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from pennylane) (3.4.2)\n",
            "Requirement already satisfied: rustworkx>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from pennylane) (0.16.0)\n",
            "Requirement already satisfied: autograd in /usr/local/lib/python3.11/dist-packages (from pennylane) (1.7.0)\n",
            "Requirement already satisfied: tomlkit in /usr/local/lib/python3.11/dist-packages (from pennylane) (0.13.2)\n",
            "Requirement already satisfied: appdirs in /usr/local/lib/python3.11/dist-packages (from pennylane) (1.4.4)\n",
            "Requirement already satisfied: autoray>=0.6.11 in /usr/local/lib/python3.11/dist-packages (from pennylane) (0.7.1)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.11/dist-packages (from pennylane) (5.5.2)\n",
            "Requirement already satisfied: pennylane-lightning>=0.40 in /usr/local/lib/python3.11/dist-packages (from pennylane) (0.40.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from pennylane) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from pennylane) (4.12.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from pennylane) (24.2)\n",
            "Requirement already satisfied: diastatic-malt in /usr/local/lib/python3.11/dist-packages (from pennylane) (2.15.2)\n",
            "Requirement already satisfied: scipy-openblas32>=0.3.26 in /usr/local/lib/python3.11/dist-packages (from pennylane-lightning>=0.40->pennylane) (0.3.29.0.0)\n",
            "Requirement already satisfied: astunparse in /usr/local/lib/python3.11/dist-packages (from diastatic-malt->pennylane) (1.6.3)\n",
            "Requirement already satisfied: gast in /usr/local/lib/python3.11/dist-packages (from diastatic-malt->pennylane) (0.6.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from diastatic-malt->pennylane) (2.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->pennylane) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->pennylane) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->pennylane) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->pennylane) (2025.1.31)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse->diastatic-malt->pennylane) (0.45.1)\n",
            "Requirement already satisfied: six<2.0,>=1.6.1 in /usr/local/lib/python3.11/dist-packages (from astunparse->diastatic-malt->pennylane) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "izOMwbJ9C0MM"
      },
      "outputs": [],
      "source": [
        "import pennylane as qml\n",
        "from pennylane import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "n_qubits = 6\n",
        "dev = qml.device(\"default.qubit\", wires=n_qubits)\n",
        "@qml.qnode(dev, interface=\"torch\")\n",
        "def quantum_convolution_mera(inputs, weights):\n",
        "    for i in range(n_qubits):\n",
        "        qml.RY(inputs[i], wires=i) # encodes data\n",
        "\n",
        "    for i in range(0, n_qubits - 1, 2):\n",
        "        qml.CNOT(wires=[i, i+1])\n",
        "        qml.RY(weights[i], wires=i)\n",
        "        qml.RY(weights[i+1], wires=i+1)\n",
        "\n",
        "    for i in range(1, n_qubits - 2, 2):\n",
        "        qml.CNOT(wires=[i, i+1])\n",
        "        qml.RY(weights[i+2], wires=i)\n",
        "\n",
        "    return [qml.expval(qml.PauliZ(i)) for i in range(n_qubits)]\n",
        "\n",
        "@qml.qnode(dev, interface=\"torch\")\n",
        "def quantum_error_correction(inputs, weights):\n",
        "    qml.RY(inputs[0], wires=0)\n",
        "    qml.CNOT(wires=[0, 1])\n",
        "    qml.CNOT(wires=[0, 2])\n",
        "\n",
        "    # Apply correction: Detect & fix errors\n",
        "    qml.CNOT(wires=[1, 3])\n",
        "    qml.CNOT(wires=[2, 4])\n",
        "\n",
        "    return [qml.expval(qml.PauliZ(i)) for i in range(5)]\n",
        "\n",
        "# replcing pooling layers based on the paper\n",
        "@qml.qnode(dev, interface=\"torch\")\n",
        "def quantum_pooling(inputs):\n",
        "    for i in range(n_qubits):\n",
        "        qml.RY(inputs[i], wires=i)\n",
        "\n",
        "    qml.CNOT(wires=[0, 1])\n",
        "    qml.CNOT(wires=[2, 3])\n",
        "    qml.CNOT(wires=[4, 5])\n",
        "\n",
        "    return [qml.expval(qml.PauliZ(0)), qml.expval(qml.PauliZ(2)), qml.expval(qml.PauliZ(4))]  # 3 values\n",
        "\n",
        "# QNN\n",
        "@qml.qnode(dev, interface=\"torch\")\n",
        "def quantum_dense(inputs, weights):\n",
        "    for i in range(len(inputs)):\n",
        "        qml.RY(inputs[i], wires=i)\n",
        "\n",
        "    for i in range(len(weights)):\n",
        "        qml.RY(weights[i], wires=i)\n",
        "\n",
        "    return qml.expval(qml.PauliZ(0))\n",
        "\n",
        "class QCNN_MERA_QEC(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(QCNN_MERA_QEC, self).__init__()\n",
        "        self.qconv_weights = nn.Parameter(torch.rand(n_qubits))\n",
        "        self.qdense_weights = nn.Parameter(torch.rand(3))  # 3 features after pooling\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = quantum_convolution_mera(x, self.qconv_weights)\n",
        "        x = quantum_error_correction(x, self.qconv_weights)\n",
        "        x = quantum_dense(x, self.qdense_weights)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = np.load('inputs.npy')\n",
        "outputs = np.load('output.npy')\n",
        "\n",
        "print(inputs.shape)\n",
        "print(outputs.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X9AY3CVAdcf3",
        "outputId": "efaee796-64b9-4550-bd37-74f472f082f7"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1256, 7, 7, 6)\n",
            "(1256, 1, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 16\n",
        "inputs_tensor = torch.tensor(inputs, dtype=torch.float64)\n",
        "output_tensor = torch.tensor(outputs, dtype=torch.float64).view(-1, 1)\n",
        "\n",
        "train_dataset = torch.utils.data.TensorDataset(inputs_tensor, output_tensor)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "print(train_dataset)\n",
        "print(train_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lpgyOahHlF0P",
        "outputId": "d135dc78-c217-42aa-e997-faeacf4a68e8"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<torch.utils.data.dataset.TensorDataset object at 0x78eaba4d3f50>\n",
            "<torch.utils.data.dataloader.DataLoader object at 0x78e9ab0d8f90>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = QCNN_MERA_QEC()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "for epoch in range(100):\n",
        "    epoch_loss = 0\n",
        "\n",
        "    for batch_inputs, batch_labels in train_loader:\n",
        "        batch_inputs = batch_inputs.view(batch_size, -1)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output = model(batch_inputs)\n",
        "        loss = criterion(output, batch_labels)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/10], Loss: {epoch_loss / len(train_loader):.4f}\")\n",
        "\n",
        "print(\"Completed ✅\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8a_cXL07JWVe",
        "outputId": "1c54090f-c5fb-4d71-e99c-a43028bc07f8"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 47.3580\n",
            "Epoch [2/10], Loss: 44.8025\n",
            "Epoch [3/10], Loss: 44.5629\n",
            "Epoch [4/10], Loss: 44.5140\n",
            "Epoch [5/10], Loss: 44.5637\n",
            "Epoch [6/10], Loss: 44.6063\n",
            "Epoch [7/10], Loss: 44.5611\n",
            "Epoch [8/10], Loss: 44.6086\n",
            "Epoch [9/10], Loss: 44.5753\n",
            "Epoch [10/10], Loss: 44.5454\n",
            "Epoch [11/10], Loss: 44.5527\n",
            "Epoch [12/10], Loss: 44.6094\n",
            "Epoch [13/10], Loss: 44.5794\n",
            "Epoch [14/10], Loss: 44.5347\n",
            "Epoch [15/10], Loss: 44.5939\n",
            "Epoch [16/10], Loss: 44.4758\n",
            "Epoch [17/10], Loss: 44.5254\n",
            "Epoch [18/10], Loss: 44.5823\n",
            "Epoch [19/10], Loss: 44.6113\n",
            "Epoch [20/10], Loss: 44.5829\n",
            "Epoch [21/10], Loss: 44.5405\n",
            "Epoch [22/10], Loss: 44.6150\n",
            "Epoch [23/10], Loss: 44.5981\n",
            "Epoch [24/10], Loss: 44.5725\n",
            "Epoch [25/10], Loss: 44.5858\n",
            "Epoch [26/10], Loss: 44.5853\n",
            "Epoch [27/10], Loss: 44.5812\n",
            "Epoch [28/10], Loss: 44.5463\n",
            "Epoch [29/10], Loss: 44.5523\n",
            "Epoch [30/10], Loss: 44.5681\n",
            "Epoch [31/10], Loss: 44.5727\n",
            "Epoch [32/10], Loss: 44.5908\n",
            "Epoch [33/10], Loss: 44.5729\n",
            "Epoch [34/10], Loss: 44.5677\n",
            "Epoch [35/10], Loss: 44.5921\n",
            "Epoch [36/10], Loss: 44.5512\n",
            "Epoch [37/10], Loss: 44.5420\n",
            "Epoch [38/10], Loss: 44.5694\n",
            "Epoch [39/10], Loss: 44.5974\n",
            "Epoch [40/10], Loss: 44.5346\n",
            "Epoch [41/10], Loss: 44.5976\n",
            "Epoch [42/10], Loss: 44.5929\n",
            "Epoch [43/10], Loss: 44.5415\n",
            "Epoch [44/10], Loss: 44.5842\n",
            "Epoch [45/10], Loss: 44.5901\n",
            "Epoch [46/10], Loss: 44.6002\n",
            "Epoch [47/10], Loss: 44.6039\n",
            "Epoch [48/10], Loss: 44.5523\n",
            "Epoch [49/10], Loss: 44.5812\n",
            "Epoch [50/10], Loss: 44.5787\n",
            "Epoch [51/10], Loss: 44.5525\n",
            "Epoch [52/10], Loss: 44.6113\n",
            "Epoch [53/10], Loss: 44.6067\n",
            "Epoch [54/10], Loss: 44.5683\n",
            "Epoch [55/10], Loss: 44.5434\n",
            "Epoch [56/10], Loss: 44.5750\n",
            "Epoch [57/10], Loss: 44.5807\n",
            "Epoch [58/10], Loss: 44.5873\n",
            "Epoch [59/10], Loss: 44.5671\n",
            "Epoch [60/10], Loss: 44.5474\n",
            "Epoch [61/10], Loss: 44.6209\n",
            "Epoch [62/10], Loss: 44.5788\n",
            "Epoch [63/10], Loss: 44.5565\n",
            "Epoch [64/10], Loss: 44.5994\n",
            "Epoch [65/10], Loss: 44.5963\n",
            "Epoch [66/10], Loss: 44.5614\n",
            "Epoch [67/10], Loss: 44.5622\n",
            "Epoch [68/10], Loss: 44.5842\n",
            "Epoch [69/10], Loss: 44.5698\n",
            "Epoch [70/10], Loss: 44.5270\n",
            "Epoch [71/10], Loss: 44.5379\n",
            "Epoch [72/10], Loss: 44.5287\n",
            "Epoch [73/10], Loss: 44.5935\n",
            "Epoch [74/10], Loss: 44.5747\n",
            "Epoch [75/10], Loss: 44.6078\n",
            "Epoch [76/10], Loss: 44.5699\n",
            "Epoch [77/10], Loss: 44.5591\n",
            "Epoch [78/10], Loss: 44.5601\n",
            "Epoch [79/10], Loss: 44.6052\n",
            "Epoch [80/10], Loss: 44.5914\n",
            "Epoch [81/10], Loss: 44.5832\n",
            "Epoch [82/10], Loss: 44.5727\n",
            "Epoch [83/10], Loss: 44.5195\n",
            "Epoch [84/10], Loss: 44.5598\n",
            "Epoch [85/10], Loss: 44.5685\n",
            "Epoch [86/10], Loss: 44.5832\n",
            "Epoch [87/10], Loss: 44.5463\n",
            "Epoch [88/10], Loss: 44.5703\n",
            "Epoch [89/10], Loss: 44.5845\n",
            "Epoch [90/10], Loss: 44.5660\n",
            "Epoch [91/10], Loss: 44.5930\n",
            "Epoch [92/10], Loss: 44.5439\n",
            "Epoch [93/10], Loss: 44.5682\n",
            "Epoch [94/10], Loss: 44.5352\n",
            "Epoch [95/10], Loss: 44.6044\n",
            "Epoch [96/10], Loss: 44.5820\n",
            "Epoch [97/10], Loss: 44.5450\n",
            "Epoch [98/10], Loss: 44.5104\n",
            "Epoch [99/10], Loss: 44.5679\n",
            "Epoch [100/10], Loss: 44.5632\n",
            "Completed ✅\n"
          ]
        }
      ]
    }
  ]
}